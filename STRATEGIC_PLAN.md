# Vibe Platform: Comprehensive Strategic Plan

**Created:** February 8, 2026
**Status:** Strategic Architecture & Roadmap
**Audience:** Development team, system architects

---

### STRATEGIC_PLAN_START ###

## VISION SUMMARY

**Vibe** is an AI-powered autonomous agent orchestration platform that evaluates ideas (IdeaIncubator) and executes complex software development tasks (ParentHarness) through coordinated multi-agent workflows. The vision is to create a self-improving, collaborative system where specialized AI agents work together to analyze problems, design solutions, implement code, validate work, and continuously improve the platform itselfâ€”ultimately enabling humans to focus on strategic decisions while agents handle execution and optimization.

---

## CURRENT STATE ASSESSMENT

### Architecture

The project consists of **three tightly integrated subsystems**:

1. **Idea Incubator** (`/ideas`, `/scripts/evaluate.ts`)
   - AI-powered idea evaluation system with specialized evaluators for Problem, Solution, Market, Feasibility, Risk, and Fit
   - Database-backed storage with markdown file sync for Q&A development
   - **Status**: Core evaluation engine works; data flow issues being fixed (Q&A sync, profile context, web research)

2. **Parent Harness** (`/parent-harness/`)
   - Orchestration framework for autonomous agent execution
   - Dashboard frontend (React 19 + Vite + Tailwind CSS 4)
   - Backend API (Express, SQLite, WebSocket, Telegram bot)
   - **Status**: 43-phase implementation plan documented; frontend shell complete (8/8 Phase 1 tasks); Phase 2-3 in progress

3. **Agent Swarm** (`/agents`, `/server/agents`, `/parent-harness/agents`)
   - 12+ specialized agents (Planning, Build, Spec, QA, Task, SIA, Research, Clarification, Human Sim, etc.)
   - Each with distinct responsibilities, model assignments, and Telegram channels
   - Autonomous task execution with CLI output tracking and intervention system
   - **Status**: Agent interfaces defined; execution framework partially implemented

### Code Quality

- **TypeScript**: Compilation passes, type coverage generally good
- **Tests**: 1773 tests across 106 test files (most passing; recent fixes for observability integration)
- **Build System**: npm monorepo with tsc + vite for frontend; successful builds
- **Database Schema**: 33 tables, migration system in place
- **Documentation**: Excellent - architecture, phases, agents, gaps analysis all documented

### Test Coverage

- Core evaluation logic: covered
- API endpoints: covered (phase-based test system)
- Database operations: covered
- Recent focus: Observability stats endpoint + request counter integration (FIXED)

### Key Gaps

| Gap | Category | Severity | Notes |
|-----|----------|----------|-------|
| Q&A data flow (fixed) | Evaluation | ðŸ”´ FIXED | development.md sync now integrated |
| Profile context incomplete (fixed) | Evaluation | ðŸŸ¡ FIXED | category-relevant excerpts implemented |
| Web search for viability (fixed) | Evaluation | ðŸŸ¡ FIXED | pre-eval research phase added |
| Clarification Agent not implemented | Execution | ðŸ”´ Critical | Users provide vague tasks â†’ wrong implementations |
| Human Sim Agent not implemented | Testing | ðŸ”´ Critical | Can't test usability with personas |
| Agent memory system incomplete | Learning | ðŸŸ  High | Agents don't learn from failures |
| Task version history & rollback | Safety | ðŸŸ¡ Medium | No checkpoint/recovery system |
| Spec workflow state machine | Architecture | ðŸŸ¡ Medium | Spec approval process missing |
| Intervention tracking partial | Audit | ðŸŸ¡ Medium | Need full SIA-style intervention tracking |
| Build interventions not tracked | Audit | ðŸŸ¡ Medium | Human/SIA fixes not recorded |
| File impact analysis (schema only) | Planning | ðŸŸ¢ Low | Schema exists, implementation deferred |
| Parallelism calculator (schema only) | Planning | ðŸŸ¢ Low | Schema exists, implementation deferred |
| Traceability PRDâ†’Code | Quality | ðŸŸ  High | Gap detection between layers missing |

---

## RECOMMENDED APPROACH

### Strategy: Progressive Enhancement with Critical Path

The 43-phase Parent Harness plan is the **critical path** to autonomous execution. The Idea Incubator is the **foundation** that informs those decisions. We'll:

1. **Stabilize & Complete Fundamentals** (Weeks 1-2)
   - Finish Idea Incubator fixes (Q&A sync, profile context, web research)
   - Complete ParentHarness Phase 1-4 (frontend shell + API)
   - Establish baseline infrastructure

2. **Build Core Agent Capabilities** (Weeks 3-4)
   - Implement critical missing agents (Clarification, Human Sim)
   - Establish memory and learning systems
   - Set up intervention tracking

3. **Enable Autonomous Execution** (Weeks 5-6)
   - Orchestrator task assignment and wave execution
   - QA validation and stuck detection
   - Telegram integration for real-time notifications

4. **Self-Improvement Loop** (Weeks 7-8)
   - Planning Agent creates improvement tasks
   - Pattern learning from failures
   - Performance monitoring and optimization

5. **Scale & Polish** (Weeks 9+)
   - Parallel execution and conflict detection
   - Advanced traceability and audit logging
   - User-facing dashboard and analytics

### Dependency Graph

```
FOUNDATION TIER (must complete first):
  â€¢ Idea Incubator fixes (Q&A sync, profile, web search)
  â€¢ ParentHarness Phases 1-2 (Frontend + Data Model)
  â€¢ Database schema finalization
  â””â”€â†’ Enables: All downstream work

INFRASTRUCTURE TIER (parallel paths):
  Path A: Agent Capabilities        Path B: API Layer
  â€¢ Clarification Agent              â€¢ Express API (Phase 3)
  â€¢ Agent memory system              â€¢ WebSocket setup (Phase 5)
  â€¢ SIA intervention tracking        â€¢ Telegram bot (Phase 6)
  â””â”€â†’ Enables: Orchestration

EXECUTION TIER:
  â€¢ Orchestrator (cron loop)
  â€¢ Task assignment algorithm
  â€¢ Wave/lane calculation
  â””â”€â†’ Enables: Autonomous work

OPTIMIZATION TIER:
  â€¢ Planning Agent
  â€¢ Technique effectiveness tracking
  â€¢ Self-improvement task creation
  â””â”€â†’ Enables: Continuous improvement
```

---

## PHASE BREAKDOWN: 8 STRATEGIC PHASES

---

### PHASE 1: Idea Incubator Finalization
**GOAL:** Evaluation system produces high-quality, context-aware assessments

**PRIORITY:** P0
**ESTIMATED_EFFORT:** medium
**DEPENDENCIES:** none
**DURATION:** 3-4 days

**KEY_DELIVERABLES:**
- âœ… Markdownâ†’Database sync for development.md Q&A
- âœ… Category-relevant profile excerpts for all evaluators
- âœ… Web research phase for Market + Solution categories
- âœ… Updated budget ($10â†’$15) to accommodate research
- âœ… Staleness detection via development.md in content hash
- âœ… All evaluation tests passing with improved scores

**TECHNICAL DETAILS:**
- **Files to modify**: `scripts/sync.ts`, `agents/specialized-evaluators.ts`, `scripts/evaluate.ts`
- **Files to create**: `agents/research.ts`, `questions/parser.ts`
- **DB changes**: Add `development.md` hash tracking to evaluation cache
- **Test validation**: `npm run evaluate <slug>` produces evidence-based results

**Success Criteria:**
1. Q&A data flows from markdown â†’ database â†’ evaluators
2. Profile context appears in Feasibility, Market, Risk evaluations
3. Market/Solution categories include web research findings
4. Evaluation scores improve from 2.3/10 â†’ 8/10 quality
5. All 6 evaluation categories pass test suite

---

### PHASE 2: ParentHarness Frontend & API Foundation (Phases 1-3)
**GOAL:** Complete frontend shell and backend REST API for data access

**PRIORITY:** P0
**ESTIMATED_EFFORT:** large
**DEPENDENCIES:** Phase 1 (independent but foundational)
**DURATION:** 5-7 days

**KEY_DELIVERABLES:**
- âœ… Dashboard pages: Dashboard, Tasks, Sessions, Config
- âœ… Agent status cards, event stream, task queue components
- âœ… React Router navigation with active state tracking
- âœ… SQLite database with 33 tables, schema migrations
- âœ… Express REST API for agents, tasks, sessions, events
- âœ… Test record system with 106 phase-based test cases
- âœ… Notification center and real-time UI updates (via mock WebSocket)

**TECHNICAL DETAILS:**
- **Frontend Stack**: React 19 + Vite + Tailwind CSS 4
- **Backend Stack**: Node.js + Express + better-sqlite3 + TypeScript
- **Database**: SQLite with migration system
- **Completed Tasks**: Phase 1 (8 tasks), Phase 2 (6 tasks), Phase 3 (7 tasks)

**Success Criteria:**
1. `npm run build` succeeds (frontend + backend)
2. `npm run dev` starts API on port 3333, dashboard on port 5173
3. All 21 test cases pass (Phase 1-3)
4. Dashboard displays mock data from API endpoints
5. Navigation works without page reloads

---

### PHASE 3: WebSocket Real-Time & Critical Missing Agents
**GOAL:** Live data updates and clarification capability for vague user tasks

**PRIORITY:** P0
**ESTIMATED_EFFORT:** large
**DEPENDENCIES:** Phase 2
**DURATION:** 5-7 days

**KEY_DELIVERABLES:**
- **WebSocket Layer** (ParentHarness Phase 5):
  - `ws://localhost:3333/ws` broadcast server
  - Event subscriptions: `agent:*`, `task:*`, `session:*`, `event:*`
  - Heartbeat/keep-alive mechanism
  - Message routing by agent ID

- **Clarification Agent** (NEW):
  - Singleton agent: `clarification_agent`
  - Triggers on all new user tasks (bypass for agent-generated tasks)
  - Uses `question_engine.ts` to generate context-aware questions
  - Blocks task execution until user responds OR timeout (24h)
  - Records Q&A in `clarification_sessions` table

- **Human Sim Agent** (NEW):
  - Multi-persona usability testing (technical, power-user, casual, confused, impatient)
  - Triggers after Build Agent completes UI tasks
  - Uses Agent Browser for automation
  - Tests happy path + error recovery per persona
  - Aggregates findings â†’ creates bug/fix tasks

**TECHNICAL DETAILS:**
- **WebSocket**: ws library, broadcast pattern with client subscriptions
- **Clarification Agent**: Sonnet model, uses existing question_engine infrastructure
- **Human Sim Agent**: Sonnet model, browser automation via Agent Browser MCP
- **Intervention tracking**: Extend `sia_attempts` pattern

**Success Criteria:**
1. WebSocket server starts and broadcasts events
2. Dashboard subscribes and displays live agent/task updates
3. Clarification Agent asks questions for vague tasks
4. Human Sim Agent tests UI with 3+ personas
5. All WebSocket + agent tests pass

---

### PHASE 4: Agent Memory & Learning System
**GOAL:** Agents learn from failures and apply successful techniques to similar problems

**PRIORITY:** P1
**ESTIMATED_EFFORT:** large
**DEPENDENCIES:** Phase 3
**DURATION:** 4-6 days

**KEY_DELIVERABLES:**
- **Long-term Agent Memory**:
  - `agent_memories` table: per-agent knowledge (decisions, failures, preferences)
  - Task signature hashing for similar-task matching
  - Relevance scoring + access tracking

- **Technique Effectiveness**:
  - `technique_effectiveness` table: error patterns â†’ success rates
  - Track which fixes work for which errors across agents
  - Auto-escalation logic after N failures

- **SIA Intervention System**:
  - Extend `sia_attempts`: each intervention logged with technique used
  - Feedback loop: success/failure recorded â†’ technique effectiveness updated
  - Matching system: similar errors â†’ suggest known-good techniques

- **Build Intervention Tracking**:
  - Record when Build Agent fixes another agent's incomplete work
  - Link intervention to specific task/session/error
  - Pattern analysis: which agents need help most

**TECHNICAL DETAILS:**
- **Tables**: `agent_memories`, `technique_effectiveness`, `sia_attempts` (extended), `build_interventions`
- **Matching Algorithm**: Levenshtein distance on error messages + semantic hashing
- **Query Performance**: Indexes on agent_id, task_signature, error_pattern

**Success Criteria:**
1. `agent_memories` table populated as agents work
2. Technique effectiveness tracked across attempts
3. SIA suggests previous solutions for known errors
4. Build Agent interventions recorded with outcomes
5. Query tests verify matching algorithm accuracy

---

### PHASE 5: Orchestrator & Task Execution Engine
**GOAL:** Autonomous task assignment, execution coordination, and stuck detection

**PRIORITY:** P0
**ESTIMATED_EFFORT:** large
**DEPENDENCIES:** Phase 2, Phase 3, Phase 4
**DURATION:** 5-7 days

**KEY_DELIVERABLES:**
- **Orchestrator Cron Loop**:
  - Runs every 60 seconds
  - Check agent heartbeats (last activity timestamp)
  - Assign ready tasks to idle agents
  - Update wave progress
  - Emit events to dashboard

- **Task Assignment Algorithm**:
  - Wave/lane calculation (find parallelizable tasks)
  - Conflict detection (file impact analysis)
  - Agent capability matching (Opus for complex, Sonnet for routine)
  - Priority-based queue management

- **QA Validation Cycle** (every 15 min):
  - Analyze CLI output for stuck agents (no activity, error loops, etc.)
  - Validate completed iterations (tests pass, lint clean, pass criteria met)
  - Record QA findings in database
  - Terminate stuck sessions with recommendations

- **Event Broadcasting**:
  - Task assigned/completed/failed
  - Agent status changes
  - Wave progress updates
  - Stuck agent detection alerts

**TECHNICAL DETAILS:**
- **Orchestrator**: Haiku model (fast, cheap), `parent-harness/orchestrator/src/orchestrator/index.ts`
- **Wave calculation**: `utils/task-pipeline.ts` (wave/lane generation)
- **File impact analysis**: Implement `file_impact_analyzer.ts` logic
- **Stuck detection**: CLI output parsing + pattern matching

**Success Criteria:**
1. Orchestrator runs every 60s without blocking
2. Tasks assigned to agents within 5 minutes of ready status
3. 80%+ accuracy in detecting stuck agents
4. No false positives on stuck detection
5. All events properly broadcast to WebSocket subscribers

---

### PHASE 6: Planning Agent & Self-Improvement Loop
**GOAL:** AI-driven continuous improvement through autonomous task creation

**PRIORITY:** P1
**ESTIMATED_EFFORT:** large
**DEPENDENCIES:** Phase 5
**DURATION:** 4-6 days

**KEY_DELIVERABLES:**
- **Planning Agent** (Singleton):
  - Opus model for strategic thinking
  - Runs on cron (every 2 hours or after major completions)
  - **Inputs**: codebase analysis, CLI logs, completed task history, failed patterns, user vision
  - **Outputs**: new tasks, bug reports, improvement suggestions, technical debt tickets
  - **Examples**:
    - "Test coverage dropped below 80% in server/services/ â†’ create unit test task"
    - "3 tasks failed due to missing type exports â†’ create barrel export task"
    - "User vision mentions collaboration â†’ create WebSocket layer task"

- **Task Creation Service**:
  - Auto-generate task IDs, PRDs, pass criteria
  - Link to origin (Planning Agent evaluation)
  - Set initial priority/category

- **Pattern Analysis**:
  - Group failed tasks by error type
  - Identify systemic issues (missing exports, type issues, etc.)
  - Prioritize based on impact

- **Vision Alignment**:
  - Read user's stated vision from `config/vision.md`
  - Compare against implemented features
  - Identify gaps and create epics

**TECHNICAL DETAILS:**
- **Model**: Opus (strategic, expensive)
- **Schedule**: Every 2 hours + after major completions
- **Inputs**: `scripts/analyze-codebase.ts` (new), CLI log parser, task history queries
- **Decision logging**: Record reasoning for each suggested task

**Success Criteria:**
1. Planning Agent runs on schedule
2. Creates 2-5 tasks per analysis cycle
3. Tasks are specific and testable (not vague)
4. 80%+ of suggested tasks eventually completed by agents
5. Dashboard shows task origin (Planning Agent vs. User vs. other agents)

---

### PHASE 7: Telegram Integration & Real-time Notifications
**GOAL:** Human-in-the-loop capability with instant alerts for important events

**PRIORITY:** P1
**ESTIMATED_EFFORT:** medium
**DEPENDENCIES:** Phase 5
**DURATION:** 3-4 days

**KEY_DELIVERABLES:**
- **Telegram Bot Service**:
  - Connect to each agent's Telegram channel
  - Post updates: task assigned, completed, failed, stuck detection
  - Include actionable summaries (error, suggestion, next steps)

- **Human Decision Hooks**:
  - Clarification requests go to user's private Telegram
  - Stuck agents trigger emergency escalation
  - Critical failures require human sign-off

- **Notification Routing**:
  - Agent-specific channels (build_agent, qa_agent, planning, etc.)
  - User notifications for urgent/escalated issues
  - Aggregated daily summary reports

**TECHNICAL DETAILS:**
- **Library**: telegram-bot-api (or similar)
- **Bot tokens**: Configured via environment variables
- **Message templates**: Markdown formatting for readability
- **Webhook/polling**: Choose integration pattern

**Success Criteria:**
1. Bot connects to all 12+ agent channels
2. Messages post within 5 seconds of event
3. User receives clarification requests
4. Stuck agent alerts trigger escalation
5. No message delivery failures

---

### PHASE 8: Advanced Features & Polishing
**GOAL:** Scale to production, add analytics, and optimize performance

**PRIORITY:** P2
**ESTIMATED_EFFORT:** large
**DEPENDENCIES:** Phase 7
**DURATION:** 7-10 days

**KEY_DELIVERABLES:**
- **Task Version History & Rollback**:
  - `task_versions` table tracking full history
  - Checkpoint/recovery system for failed executions
  - Diff tracking between versions

- **Traceability Service**:
  - PRD â†’ Spec â†’ Task â†’ Code â†’ Test linking
  - Gap detection between layers
  - Coverage metrics dashboard

- **Acceptance Criteria Results**:
  - Per-criterion pass/fail tracking (extend pass_criteria JSON)
  - Detailed results for each evaluation point
  - Historical trend analysis

- **Spec Workflow State Machine**:
  - Spec approval process before task creation
  - Stakeholder sign-off capability
  - Version control for specs

- **File Conflict Detection**:
  - Implement `file_conflict_detector.ts`
  - Prevent parallel agents from editing same file
  - Suggest lane/wave adjustments

- **Performance Optimization**:
  - Database query optimization (indexing)
  - API response time reduction
  - Memory footprint analysis for long-running sessions

- **Analytics Dashboard**:
  - Agent productivity metrics (tasks/day, success rate)
  - System health indicators (uptime, error rates)
  - Cost tracking (token usage, API calls)
  - User satisfaction scores

**TECHNICAL DETAILS:**
- Build on all previous phases
- Extend existing schema minimally
- Focus on query optimization + UI/UX polish

**Success Criteria:**
1. Task version history tracks all changes
2. Traceability view shows PRDâ†”Code links
3. File conflicts prevented in 95%+ of cases
4. Analytics dashboard displays in real-time
5. System handles 50+ concurrent tasks without degradation

---

## CROSS-CUTTING CONCERNS

### Monitoring & Observability

- **Existing**: Request counter, observability stats endpoint (recently fixed)
- **Add**: Request/response logging, error categorization, performance metrics
- **Dashboard**: Real-time health view (agent status, task queue depth, error rate)

### Security

- **Auth**: Not implemented yet (out of scope for v1)
- **Database**: SQLite (suitable for single-machine; upgrade path for multi-machine)
- **API**: CORS configured, input validation needed
- **Secrets**: Environment variables for Telegram tokens, API keys

### Testing Strategy

- **Unit**: Jest for core services (task assignment, wave calculation)
- **Integration**: API endpoint tests with database
- **E2E**: CLI simulation with multi-agent coordination
- **Phase gates**: 106 test cases across 16 phases (already planned)

### Error Handling & Recovery

- **Graceful degradation**: Agents continue work even if one fails
- **Automatic retry**: Failed API calls with exponential backoff
- **Human escalation**: Stuck detection â†’ Telegram alert â†’ manual intervention
- **Rollback capability**: Task version history enables recovery

---

## TIMELINE SUMMARY

| Phase | Name | Duration | Team Size | Key Risk |
|-------|------|----------|-----------|----------|
| 1 | Idea Incubator Fixes | 3-4 days | 1 agent | Data migration completeness |
| 2 | Frontend & API Foundation | 5-7 days | 2 agents (frontend + backend) | Complexity of phase system |
| 3 | WebSocket & Critical Agents | 5-7 days | 3 agents | Agent capability requirements |
| 4 | Memory & Learning | 4-6 days | 2 agents | Matching algorithm accuracy |
| 5 | Orchestrator & Execution | 5-7 days | 3 agents | Stuck detection false positives |
| 6 | Planning Agent | 4-6 days | 1 agent | Quality of generated tasks |
| 7 | Telegram Integration | 3-4 days | 1 agent | API rate limits |
| 8 | Polish & Analytics | 7-10 days | 2 agents | Integration complexity |
| **TOTAL** | | **36-51 days** | 15 agent-days | Multiple parallel paths |

### Parallelization Opportunities

- **Phase 2**: Frontend and backend API can be built in parallel (separate concerns)
- **Phase 3**: WebSocket and agents can advance independently
- **Phase 4**: Memory system development parallel with orchestrator work
- **Phase 6**: Planning Agent developed while Phase 5 Orchestrator is completing
- **Estimated acceleration**: 36-51 days â†’ 25-35 days with 3-4 parallel teams

---

## SUCCESS METRICS

### Phase Completion

- **Idea Incubator Quality**: Evaluation scores improve from 2.3/10 â†’ 8/10
- **Frontend & API**: All 21 tests pass; dashboard fully functional
- **Real-time Updates**: WebSocket latency <500ms; 99% delivery
- **Agent Autonomy**: Clarification Agent handles 100% of vague tasks; Human Sim covers 80% of UI
- **Task Execution**: 90%+ of assigned tasks complete successfully
- **Self-Improvement**: Planning Agent creates 10+ actionable tasks/week

### System Health

- **Uptime**: 99.5%+ (allow for maintenance restarts)
- **Error Rate**: <1% of tasks fail due to infrastructure (bugs don't count)
- **Agent Efficiency**: Average task completion time reduces by 30% week-over-week
- **Cost**: Token usage optimized to <$0.50 per task evaluation + execution

### User Experience

- **Clarity**: No ambiguous tasks reach execution (Clarification Agent solves 100%)
- **Quality**: No releases with critical bugs (QA Agent catches 95%+)
- **Responsiveness**: Telegram alerts within 30 seconds of events
- **Trustworthiness**: Users feel confident in automated decisions (survey: 8/10+)

---

## Risk Mitigation

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|-----------|
| WebSocket stability issues | Medium | High | Early testing, fallback to polling |
| Clarification Agent doesn't ask good questions | Medium | Medium | Manual question curation, feedback loop |
| Stuck detection has false positives | High | Medium | Conservative thresholds, human review |
| Agent memory matching too expensive | Medium | Medium | Incremental indexing, cache queries |
| Orchestrator creates task conflicts | Medium | High | Conservative lane allocation, validation |
| Planning Agent creates low-quality tasks | Medium | Medium | Seed with high-quality examples, review |
| Telegram API rate limits | Low | Low | Batch messages, fallback to polling |

---

## Post-Launch Roadmap (Future Work)

### Phase 9-16 (Per Existing Plan)
The 43-phase ParentHarness plan includes:
- Phase 8: UI polish + Config page
- Phase 9: Validation Agent improvements
- Phase 10-13: Advanced agents (Spawner, Memory graph, QA, Human Sim)
- Phase 14-16: Self-improvement loop, final polish

### Estimated Timeline: Phases 9-16 could complete in 30-40 additional days with same team

### Strategic Extensions (v2.0+)
- Multi-agent debates for contentious decisions
- User feedback loop integration
- Cost optimization (model selection per task)
- Knowledge export (lessons learned â†’ documentation)
- API for external clients to use Vibe for their own projects

---

### STRATEGIC_PLAN_END ###

---

## Implementation Notes for Agents

### How to Use This Plan

1. **Phase Selection**: Start with Phase 1 (Idea Incubator fixes). These are prerequisites for trustworthy evaluations.
2. **Parallel Tracks**: Phases 2-3 can run in parallel once Phase 1 starts. Phase 4-5 can advance once Phase 2 is complete.
3. **Team Assignment**:
   - **Build Agent**: Code implementation (Phases 2, 5, 7)
   - **Spec Agent**: Document PRDs and requirements (Phases 1, 6, 8)
   - **QA Agent**: Testing and validation (all phases)
   - **Planning Agent**: Strategic oversight (Phase 6 onwards)
   - **SIA**: Ideation and arbitration (cross-cutting)

4. **Phase Gates**: Do NOT start a phase until all dependencies are complete.
5. **Risk Check**: Before Phase 5 (Orchestrator), ensure WebSocket is stable and agents can respond.

### Escalation Path

- **Stuck on technical issue**: Escalate to Build Agent + QA Agent
- **Architectural question**: Escalate to Planning Agent + Spec Agent
- **User decision needed**: Route to Telegram user notifications
- **Critical production issue**: Page all agents, coordinator decides

---

## Conclusion

This plan provides a **clear, testable roadmap** for building Vibe from its current strong foundation into a fully autonomous, self-improving system. The critical path is:

**Idea Incubator â†’ Foundation API/Frontend â†’ Orchestrator â†’ Self-Improvement Loop**

Each phase builds on prior work with clear success metrics. With proper parallelization and team coordination, the entire platform can reach v1.0 production readiness in **6-8 weeks** with 3-4 concurrent agent teams.

The design prioritizes **reliability** (extensive testing), **observability** (logging and metrics), and **human control** (escalation paths) over raw speed. This ensures the system remains trustworthy as it grows.

---

**Next Step**: Approval to begin Phase 1 (Idea Incubator fixes) with Build Agent, Spec Agent, and QA Agent.
