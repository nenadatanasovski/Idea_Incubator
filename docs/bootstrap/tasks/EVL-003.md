# EVL-003: Debate Round Orchestration

---

## Metadata

| Field          | Value                 |
| -------------- | --------------------- |
| **Phase**      | 4 - Evaluation System |
| **Depends On** | EVL-002               |
| **Blocks**     | EVL-004               |
| **Priority**   | P3                    |
| **Owner**      | Human + Claude Code   |

---

## Summary

Orchestrate structured debate rounds where evaluation agents argue different positions on the idea. An arbiter judges each round and determines outcomes that affect final scores.

---

## Context

### Debate Structure

```
Round 1: Problem Validity
  Advocate: "Problem is real and significant"
  Challenger: "Problem is overstated or niche"
  Arbiter: Judge arguments, adjust Problem scores

Round 2: Solution Fit
  Advocate: "Solution effectively addresses problem"
  Challenger: "Solution has critical flaws"
  Arbiter: Judge arguments, adjust Solution scores

Round 3: Market Opportunity
  Advocate: "Market is large and accessible"
  Challenger: "Market is limited or saturated"
  Arbiter: Judge arguments, adjust Market scores
```

### Debate Roles

| Role           | Purpose                      |
| -------------- | ---------------------------- |
| **Advocate**   | Defend the idea's strengths  |
| **Challenger** | Attack weaknesses (red team) |
| **Arbiter**    | Judge arguments fairly       |

---

## Requirements

1. **Debate Engine**:
   - Configure debate topics
   - Generate advocate/challenger arguments
   - Run arbiter judgment

2. **Argument Generation**:
   - Based on evaluation evidence
   - Structured reasoning
   - Counter-argument awareness

3. **Arbiter Logic**:
   - Fair evaluation of both sides
   - Identify strongest arguments
   - Provide score adjustments

4. **Score Integration**:
   - Debate outcomes affect final scores
   - Track pre/post debate deltas
   - Document reasoning

---

## Pass Criteria

**PASS** when ALL of the following are true:

| #   | Criterion                 | How to Verify           |
| --- | ------------------------- | ----------------------- |
| 1   | 3 debate rounds run       | Check debate output     |
| 2   | Arguments are substantive | Review argument quality |
| 3   | Arbiter provides judgment | Each round has verdict  |
| 4   | Scores updated            | Compare pre/post debate |

**FAIL** if any criterion is not met.

---

## Output Files

```
agents/debate.ts                           <- Debate orchestrator
agents/arbiter.ts                          <- Judgment logic
types/debate.ts                            <- TypeScript types
tests/unit/agents/debate.test.ts
```

---

## Code Template

```typescript
// agents/debate.ts

interface DebateRound {
  topic: string;
  targetCategory: string;
  advocatePosition: string;
  challengerPosition: string;
  advocateArguments: Argument[];
  challengerArguments: Argument[];
  arbiterVerdict: Verdict;
  scoreAdjustments: Record<string, number>;
}

interface Verdict {
  winner: "advocate" | "challenger" | "draw";
  reasoning: string;
  keyPoints: string[];
  confidence: number;
}

export async function runDebate(
  ideaContext: IdeaContext,
  evaluation: EvaluationResult,
  redTeamChallenges: Challenge[],
): Promise<DebateResult> {
  const rounds: DebateRound[] = [];

  for (const topic of DEBATE_TOPICS) {
    // Generate arguments
    const advocateArgs = await generateAdvocateArguments(topic, ideaContext);
    const challengerArgs = await generateChallengerArguments(
      topic,
      redTeamChallenges,
    );

    // Run arbiter judgment
    const verdict = await judgeRound(advocateArgs, challengerArgs, topic);

    rounds.push({
      topic: topic.name,
      targetCategory: topic.category,
      advocateArguments: advocateArgs,
      challengerArguments: challengerArgs,
      arbiterVerdict: verdict,
      scoreAdjustments: calculateAdjustments(verdict, topic.category),
    });
  }

  return { rounds, finalAdjustments: aggregateAdjustments(rounds) };
}
```

---

## Validation

```bash
# Run debate tests
npm test -- tests/unit/agents/debate.test.ts

# Run full evaluation with debate
npm run evaluate test-idea -- --debate

# Review debate transcript
cat users/default/ideas/test-idea/analysis/debate.md
```

---

## Next Steps

After completing: Generate synthesis document (EVL-004).
